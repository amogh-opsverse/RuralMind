from fastapi import FastAPI, File, UploadFile
from fastapi import Form
from typing import Dict
import re
import pandas as pd
#from unidecode import unidecode
import numpy as np
import io
import os
from langchain import PromptTemplate, HuggingFaceHub, LLMChain
from langchain.llms import HuggingFacePipeline
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSeq2SeqLM
from transformers import ReformerModelWithLMHead
from transformers import pipeline, set_seed
from pydantic import BaseModel
from fastapi.middleware.cors import CORSMiddleware


"""
backend server functions:
handles requests from the frontend in the form of context (subject) and prompt (question)
responds with an answer that is generated by the language model (should be fine tuned to the subject)
"""


app = FastAPI()

origins = [
    "http://localhost:5173",  # React server address
    "http://127.0.0.1:5173"   # Also the React server address
]

app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'

template = """Question: {question}

Answer: Let's think step by step."""

prompt = PromptTemplate(template=template, input_variables=["question"])

class Question(BaseModel):
    question: str
    topic: str


def scienceModel(prompt):
  model_id = 'google/flan-t5-large'
  tokenizer = AutoTokenizer.from_pretrained(model_id)

  #model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=True, device_map='auto', load_in_8bit_fp32_cpu_offload=True)
  model = AutoModelForSeq2SeqLM.from_pretrained(model_id, load_in_8bit=False, device_map='auto')


  pipe = pipeline(
      "text2text-generation",
      model=model, 
      tokenizer=tokenizer, 
      max_length=100
  )

  local_llm = HuggingFacePipeline(pipeline=pipe)

  #print(local_llm('How many planets are in the solar system?'))
  print(local_llm(prompt))
  response = local_llm(prompt)
  return response


"""reformer"""
def history(prompt):
  #model_id = 'google/reformer-enwik8'
  model = ReformerModelWithLMHead.from_pretrained("google/reformer-enwik8")
  # Encoding
  def encode(list_of_strings, pad_token_id=0):
      max_length = max([len(string) for string in list_of_strings])

      # create emtpy tensors
      attention_masks = torch.zeros((len(list_of_strings), max_length), dtype=torch.long)
      input_ids = torch.full((len(list_of_strings), max_length), pad_token_id, dtype=torch.long)

      for idx, string in enumerate(list_of_strings):
          # make sure string is in byte format
          if not isinstance(string, bytes):
              string = str.encode(string)

          input_ids[idx, :len(string)] = torch.tensor([x + 2 for x in string])
          attention_masks[idx, :len(string)] = 1

      return input_ids, attention_masks
      
  # Decoding
  def decode(outputs_ids):
      decoded_outputs = []
      for output_ids in outputs_ids.tolist():
          # transform id back to char IDs < 2 are simply transformed to ""
          decoded_outputs.append("".join([chr(x - 2) if x > 1 else "" for x in output_ids]))
      return decoded_outputs


  #encoded, attention_masks = encode(["India gained her independence in the year..."])
  encoded, attention_masks = encode([prompt])
  #print(decode(model.generate(encoded, do_sample=True, max_length=150)))
  response = decode(model.generate(encoded, do_sample=True, max_length=150))
  return response

'''conversation using gpt2-xl'''
def literature(prompt):
  generator = pipeline('text-generation', model='gpt2-xl')
  set_seed(42)
  #print(generator("The father of Indian independence is: ", max_length=30, num_return_sequences=5)[-1])
  print(generator(prompt, max_length=30, num_return_sequences=5)[-1])
  response = generator(prompt, max_length=30, num_return_sequences=5)[-1]
  return response


#set context for the model based on the loaded page using useEffect
#this will handle single questions to the gpt model
#the response can be a dictionary of the question and the answer
@app.post("/ask-gpt", response_model=Dict[str, str])
async def extract_work_text(question: Question):
    print("question from client:", question.question)
    print("topic from client:", question.topic)
    topic = question.topic
    userQ = question.question
    res = ""
    if topic == "science":
      res = scienceModel(userQ)
    elif topic == "history":
      res = scienceModel(userQ)
    elif topic == "literature":
      #res = literature(userQ)
      res = scienceModel(userQ)

    return {"response": res}